{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>tablegpt-agent is a pre-built agent for TableGPT2 (huggingface), a series of LLMs for table-based question answering. This agent is built on top of the Langgraph library and provides a user-friendly interface for interacting with TableGPT2.</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<ul> <li>Tutorials<ul> <li>Quickstart</li> <li>Chat on Tabular Data</li> <li>Continue Analysis on Generated Charts</li> </ul> </li> <li>How-To Guides<ul> <li>Enhance TableGPT Agent with RAG</li> <li>Persist Messages</li> <li>Incluster Code Execution</li> <li>Normalize Datasets</li> </ul> </li> <li>Explanation<ul> <li>Agent Workflow</li> <li>File Reading</li> </ul> </li> <li>Reference</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Thank you for your interest in TableGPT Agent. For more information on contributing, please see the contributing guide.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We extend our sincere gratitude to all contributors and collaborators who played a pivotal role in the development of tablegpt-agent. Special thanks to our team members and the open-source community, whose insights and feedback were invaluable throughout the project.</p> <p>Thank you to our early users for their suggestions and engagement, which have greatly helped in refining and enhancing this tool.</p>"},{"location":"reference/","title":"API Reference","text":"<p>Creates a state graph for processing datasets.</p> <p>This function orchestrates the creation of a workflow for handling table data. It sets up nodes for reading files and analyzing data based on provided parameters. The graph dynamically routes based on the presence of file attachments in the input state.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Runnable</code> <p>The primary language model for processing user input.</p> required <code>pybox_manager</code> <code>BasePyBoxManager</code> <p>A python code sandbox delegator, used to execute the data analysis code generated by llm.</p> required <code>session_id</code> <code>str | None</code> <p>An optional session identifier used to associate with <code>pybox</code>. Defaults to None.</p> <code>None</code> <code>workdir</code> <code>Path | None</code> <p>The working directory for <code>pybox</code> operations. Defaults to None.</p> <code>None</code> <code>error_trace_cleanup</code> <code>bool</code> <p>Flag to clean up error traces. Defaults to False.</p> <code>False</code> <code>nlines</code> <code>int | None</code> <p>Number of lines to read for preview. Defaults to None.</p> <code>None</code> <code>vlm</code> <code>BaseLanguageModel | None</code> <p>Optional vision language model for processing images. Defaults to None.</p> <code>None</code> <code>safety_llm</code> <code>Runnable | None</code> <p>Model used for safety classification of inputs. Defaults to None.</p> <code>None</code> <code>dataset_retriever</code> <code>BaseRetriever | None</code> <p>Component to retrieve datasets. Defaults to None.</p> <code>None</code> <code>normalize_llm</code> <code>BaseLanguageModel | None</code> <p>Model for data normalization tasks. Defaults to None.</p> <code>None</code> <code>locate</code> <code>str | None</code> <p>The locale of the user. Defaults to None.</p> required <code>checkpointer</code> <code>BaseCheckpointSaver | None</code> <p>Component for saving checkpoints. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Flag to enable verbose logging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>CompiledStateGraph</code> <code>CompiledStateGraph</code> <p>A compiled state graph representing the table processing workflow.</p> Source code in <code>src/tablegpt/agent/__init__.py</code> <pre><code>def create_tablegpt_graph(\n    llm: BaseLanguageModel,\n    pybox_manager: BasePyBoxManager,\n    *,\n    session_id: str | None = None,\n    workdir: Path | None = None,\n    error_trace_cleanup: bool = False,\n    nlines: int | None = None,\n    vlm: BaseLanguageModel | None = None,\n    safety_llm: Runnable | None = None,\n    dataset_retriever: BaseRetriever | None = None,\n    normalize_llm: BaseLanguageModel | None = None,\n    locale: str | None = None,\n    checkpointer: BaseCheckpointSaver | None = None,\n    verbose: bool = False,\n) -&gt; CompiledStateGraph:\n    \"\"\"Creates a state graph for processing datasets.\n\n    This function orchestrates the creation of a workflow for handling table data.\n    It sets up nodes for reading files and analyzing data based on provided parameters.\n    The graph dynamically routes based on the presence of file attachments in the input state.\n\n    Args:\n        llm (Runnable): The primary language model for processing user input.\n        pybox_manager (BasePyBoxManager):  A python code sandbox delegator, used to execute the data analysis code generated by llm.\n        session_id (str | None, optional): An optional session identifier used to associate with `pybox`. Defaults to None.\n        workdir (Path | None, optional): The working directory for `pybox` operations. Defaults to None.\n        error_trace_cleanup (bool, optional): Flag to clean up error traces. Defaults to False.\n        nlines (int | None, optional): Number of lines to read for preview. Defaults to None.\n        vlm (BaseLanguageModel | None, optional): Optional vision language model for processing images. Defaults to None.\n        safety_llm (Runnable | None, optional): Model used for safety classification of inputs. Defaults to None.\n        dataset_retriever (BaseRetriever | None, optional): Component to retrieve datasets. Defaults to None.\n        normalize_llm (BaseLanguageModel | None, optional): Model for data normalization tasks. Defaults to None.\n        locate (str | None, optional): The locale of the user. Defaults to None.\n        checkpointer (BaseCheckpointSaver | None, optional): Component for saving checkpoints. Defaults to None.\n        verbose (bool, optional): Flag to enable verbose logging. Defaults to False.\n\n    Returns:\n        CompiledStateGraph: A compiled state graph representing the table processing workflow.\n    \"\"\"\n    workflow = StateGraph(AgentState)\n    file_reading_graph = create_file_reading_workflow(\n        nlines=nlines,\n        llm=llm,\n        pybox_manager=pybox_manager,\n        workdir=workdir,\n        session_id=session_id,\n        normalize_llm=normalize_llm,\n        locale=locale,\n        verbose=verbose,\n    )\n    data_analyze_graph = create_data_analyze_workflow(\n        llm=llm,\n        pybox_manager=pybox_manager,\n        workdir=workdir,\n        session_id=session_id,\n        error_trace_cleanup=error_trace_cleanup,\n        vlm=vlm,\n        safety_llm=safety_llm,\n        dataset_retriever=dataset_retriever,\n        verbose=verbose,\n    )\n\n    def router(state: AgentState) -&gt; str:\n        # Must have at least one message when entering this router\n        last_message = state[\"messages\"][-1]\n        if last_message.additional_kwargs.get(\"attachments\"):\n            return \"file_reading_graph\"\n        return \"data_analyze_graph\"\n\n    workflow.add_node(\"file_reading_graph\", file_reading_graph)\n    workflow.add_node(\"data_analyze_graph\", data_analyze_graph)\n\n    workflow.add_conditional_edges(START, router)\n    workflow.add_edge(\"file_reading_graph\", END)\n    workflow.add_edge(\"data_analyze_graph\", END)\n\n    return workflow.compile(checkpointer=checkpointer, debug=verbose)\n</code></pre>"},{"location":"explanation/agent-workflow/","title":"Agent Workflow","text":"<p>The Agent Workflow is the core functionality of the <code>tablegpt-agent</code>. It processes user input and generates appropriate responses. This workflow is similar to those found in most single-agent systems and consists of an agent and various tools. Specifically, the data analysis workflow includes:</p> <ul> <li>An Agent Powered by TableGPT2: This agent performs data analysis tasks.</li> <li>An IPython tool: This tool executes the generated code within a sandbox environment.</li> </ul> <p>Additionally, TableGPT Agent offers several optional plugins that extend the agent's functionality:</p> <ul> <li>A Visual Language Model that can be used to enhance summarization for data visualization tasks.</li> <li>A retriever that fetches information about the dataset, improving the quality and relevance of the generated code.</li> <li>A safety mechanism that protects the system from toxic inputs.</li> </ul>"},{"location":"explanation/file-reading/","title":"File Reading","text":"<p>TableGPT Agent separates the file reading workflow from the data analysis workflow to maintain greater control over how the LLM inspects the dataset files. Typically, if you let the LLM inspect the dataset itself, it uses the <code>df.head()</code> function to preview the data. While this is sufficient for basic cases, we have implemented a more structured approach by hard-coding the file reading workflow into several steps:</p> <ul> <li><code>normalization</code> (optional): For some Excel files, the content may not be 'pandas-friendly'. We include an optional normalization step to transform the Excel content into a more suitable format for pandas.</li> <li><code>df.info()</code>: Unlike <code>df.head()</code>, <code>df.info()</code> provides insights into the dataset's structure, such as the data types of each column and the number of non-null values, which also indicates whether a column contains NaN. This insight helps the LLM understand the structure and quality of the data.</li> <li><code>df.head()</code>: The final step displays the first n rows of the dataset, where n is configurable. A larger value for n allows the LLM to glean more information from the dataset; however, too much detail may divert its attention from the primary task.</li> </ul>"},{"location":"howto/incluster-code-execution/","title":"Incluster Code Execution","text":"<p>The <code>tablegpt-agent</code> directs <code>tablegpt</code> to generate Python code for data analysis. This code is then executed within a sandbox environment to ensure system security. The execution is managed by the pybox library, which provides a simple way to run Python code outside the main process.</p>"},{"location":"howto/incluster-code-execution/#usage","title":"Usage","text":"<p>If you're using the local executor (pybox.LocalPyBoxManager), follow these steps to configure the environment:</p> <ol> <li> <p>Install the dependencies required for the <code>IPython Kernel</code> using the following command:</p> <pre><code>pip install -r ipython/requirements.txt\n</code></pre> </li> <li> <p>Copy the code from the <code>ipython/ipython-startup-scripts</code> folder to the <code>$HOME/.ipython/profile_default/startup/</code> directory.</p> <p>This folder contains the functions and configurations needed to perform data analysis with <code>tablegpt-agent</code>.</p> <p>Note: The <code>~/.ipython</code> directory must be writable for the process launching the kernel, otherwise there will be a warning message: <code>UserWarning: IPython dir '/home/jovyan/.ipython' is not a writable location, using a temp directory.</code> and the startup scripts won't take effects.</p> </li> </ol>"},{"location":"howto/normalize-datasets/","title":"Normalize Datasets","text":"<p>The <code>Dataset Normalizer</code> plugin is used to transform 'pandas-unfriendly' datasets (e.g., Excel files that do not follow a standard tabular structure) into a more suitable format for pandas. It is backed by an LLM that generates Python code to convert the original datasets into new ones.</p> <p>In <code>tablegpt-agent</code>, this plugin is used to better format 'pandas-unfriendly' datasets, making them more understandable for the subsequent steps. This plugin is optional; if used, it serves as the very first step in the File Reading workflow, easing the difficulty of data analysis in the subsequent workflow.</p>"},{"location":"howto/persist-messages/","title":"Persist Messages","text":""},{"location":"howto/retrieval/","title":"Enhance TableGPT Agent with RAG","text":"<p>While the File Reading Workflow is adequate for most scenarios, it may not always provide the information necessary for the LLM to generate accurate code. Consider the following examples:</p> <ul> <li>A categorical column in the dataset contains 'foo', 'bar', and 'baz', but 'baz' only appears after approximately 100 rows. In this case, the LLM may not encounter the 'baz' value through <code>df.head()</code>.</li> <li>The user's query may not align with the dataset's content for several reasons:</li> <li>The dataset lacks proper governance. For instance, a cell value might be misspelled from 'foo' to 'fou'.</li> <li>There could be a typo in the user's query. For example, if the user queries, \"Show me the data for 'fou',\" but the dataset contains 'foo' instead.</li> </ul> <p>In such situations, the Dataset Retriever plugin can be utilized to fetch additional information about the dataset from external sources, thereby providing the LLM with more context and improving its ability to generate accurate responses.</p>"},{"location":"tutorials/chat-on-tabular-data/","title":"Chat on Tabular Data","text":"<p>Beyond simple conversations, <code>tablegpt-agent</code> can also analyze and process tablular data. For data analysis, preprocess the data first with TableGPT. Use a fixed session_id to keep the agent within the same execution context, and set a save point to enable memory retention:</p> <pre><code>&gt;&gt;&gt; from langgraph.checkpoint.memory import MemorySaver\n\n&gt;&gt;&gt; agent = create_tablegpt_graph(\n...     llm=llm,\n...     pybox_manager=pybox_manager,\n...     checkpointer=MemorySaver(),\n...     session_id=\"some-session-id\",\n... )\n</code></pre> <p>Add the file for processing in the additional_kwargs of HumanMessage. Here's an example using the Titanic dataset.</p> <pre><code>&gt;&gt;&gt; from typing import TypedDict\n&gt;&gt;&gt; from langchain_core.messages import HumanMessage\n\n&gt;&gt;&gt; class Attachment(TypedDict):\n...     \"\"\"Contains at least one dictionary with the key filename.\"\"\"\n...     filename: str\n\n&gt;&gt;&gt; attachment_msg = HumanMessage(\n...     content=\"\",\n...     # Please make sure your iPython kernel can access your filename.\n...     additional_kwargs={\"attachments\": [Attachment(filename=\"titanic.csv\")]},\n... )\n</code></pre> <p>Invoke the agent as shown in the quick start:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from datetime import date\n&gt;&gt;&gt; from tablegpt.agent.file_reading import Stage\n\n&gt;&gt;&gt; # Reading and processing files.\n&gt;&gt;&gt; response = asyncio.run(\n...     agent.ainvoke(\n...         input={\n...             \"entry_message\": attachment_msg,\n...             \"processing_stage\": Stage.UPLOADED,\n...             \"messages\": [attachment_msg],\n...             \"parent_id\": \"some-parent-id1\",\n...             \"date\": date.today(),\n...         },\n...         config={\n...             # Using checkpointer requires binding thread_id at runtime.\n...             \"configurable\": {\"thread_id\": \"some-thread-id\"},\n...         },\n...     )\n... )\n&gt;&gt;&gt; print(response[\"messages\"])\n[HumanMessage(content='', additional_kwargs={'attachments': [{'filename': 'titanic.csv'}]}, response_metadata={}, id='9cdbb5f5-3108-4abf-a782-836b92788e82'), AIMessage(content=\"\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u6570\u636e\u6587\u4ef6\uff0c\u6211\u9700\u8981\u67e5\u770b\u6587\u4ef6\u5185\u5bb9\u4ee5\u5bf9\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u521d\u6b65\u7684\u4e86\u89e3\u3002\u9996\u5148\u6211\u4f1a\u8bfb\u53d6\u6570\u636e\u5230 `df` \u53d8\u91cf\u4e2d\uff0c\u5e76\u901a\u8fc7 `df.info` \u67e5\u770b NaN \u60c5\u51b5\u548c\u6570\u636e\u7c7b\u578b\u3002\\n```python\\n# Load the data into a DataFrame\\ndf = read_df('titanic.csv')\\n\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how='all').dropna(axis=1, how='all')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\\n```\", additional_kwargs={'parent_id': 'some-parent-id1', 'thought': '\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u6570\u636e\u6587\u4ef6\uff0c\u6211\u9700\u8981\u67e5\u770b\u6587\u4ef6\u5185\u5bb9\u4ee5\u5bf9\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u521d\u6b65\u7684\u4e86\u89e3\u3002\u9996\u5148\u6211\u4f1a\u8bfb\u53d6\u6570\u636e\u5230 `df` \u53d8\u91cf\u4e2d\uff0c\u5e76\u901a\u8fc7 `df.info` \u67e5\u770b NaN \u60c5\u51b5\u548c\u6570\u636e\u7c7b\u578b\u3002', 'action': {'tool': 'python', 'tool_input': \"# Load the data into a DataFrame\\ndf = read_df('titanic.csv')\\n\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how='all').dropna(axis=1, how='all')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\"}, 'model_type': None}, response_metadata={}, id='463f1fab-5b5e-4811-a923-b1a31c6b825c', tool_calls=[{'name': 'python', 'args': {'query': \"# Load the data into a DataFrame\\ndf = read_df('titanic.csv')\\n\\n# Remove leading and trailing whitespaces in column names\\ndf.columns = df.columns.str.strip()\\n\\n# Remove rows and columns that contain only empty values\\ndf = df.dropna(how='all').dropna(axis=1, how='all')\\n\\n# Get the basic information of the dataset\\ndf.info(memory_usage=False)\"}, 'id': 'bfd54a9f-ddf8-45fc-90b0-3d669f4e63ca', 'type': 'tool_call'}]), ToolMessage(content=[{'type': 'text', 'text': \"```pycon\\n&lt;class 'pandas.core.frame.DataFrame'&gt;\\nRangeIndex: 4 entries, 0 to 3\\nData columns (total 8 columns):\\n #   Column    Non-Null Count  Dtype  \\n---  ------    --------------  -----  \\n 0   Pclass    4 non-null      int64  \\n 1   Sex       4 non-null      object \\n 2   Age       4 non-null      float64\\n 3   SibSp     4 non-null      int64  \\n 4   Parch     4 non-null      int64  \\n 5   Fare      4 non-null      float64\\n 6   Embarked  4 non-null      object \\n 7   Survived  4 non-null      int64  \\ndtypes: float64(2), int64(4), object(2)\\n```\"}], name='python', id='34d1ab80-c742-49c8-a4a5-aa449a3f6ca3', tool_call_id='bfd54a9f-ddf8-45fc-90b0-3d669f4e63ca', artifact=[]), AIMessage(content='\u63a5\u4e0b\u6765\u6211\u5c06\u7528 `df.head(5)` \u6765\u67e5\u770b\u6570\u636e\u96c6\u7684\u524d 5 \u884c\u3002\\n```python\\n# Show the first 5 rows to understand the structure\\ndf.head(5)\\n```', additional_kwargs={'parent_id': 'some-parent-id1', 'thought': '\u63a5\u4e0b\u6765\u6211\u5c06\u7528 `df.head(5)` \u6765\u67e5\u770b\u6570\u636e\u96c6\u7684\u524d 5 \u884c\u3002', 'action': {'tool': 'python', 'tool_input': '# Show the first 5 rows to understand the structure\\ndf.head(5)'}, 'model_type': None}, response_metadata={}, id='9bfa426f-f42a-433f-944f-020fc88273ad', tool_calls=[{'name': 'python', 'args': {'query': '# Show the first 5 rows to understand the structure\\ndf.head(5)'}, 'id': '9f933c9c-4f6e-4632-a545-cbf9fb96d692', 'type': 'tool_call'}]), ToolMessage(content=[{'type': 'text', 'text': '```pycon\\n   Pclass     Sex   Age  SibSp  Parch    Fare Embarked  Survived\\n0       2  female  29.0      0      2  23.000        S         1\\n1       3  female  39.0      1      5  31.275        S         0\\n2       3    male  26.5      0      0   7.225        C         0\\n3       3    male  32.0      0      0  56.496        S         1\\n```'}], name='python', id='b1864c77-fc20-45aa-88b1-653478110dde', tool_call_id='9f933c9c-4f6e-4632-a545-cbf9fb96d692', artifact=[]), AIMessage(content='\u6211\u5df2\u7ecf\u4e86\u89e3\u4e86\u6570\u636e\u96c6 titanic.csv \u7684\u57fa\u672c\u4fe1\u606f\u3002\u8bf7\u95ee\u6211\u53ef\u4ee5\u5e2e\u60a8\u505a\u4e9b\u4ec0\u4e48\uff1f', additional_kwargs={'parent_id': 'some-parent-id1'}, response_metadata={}, id='05470ae0-22aa-4584-8b56-fcff4087d9e1')]\n</code></pre> <p>Continue to ask questions for data analysis:</p> <pre><code>&gt;&gt;&gt; human_message = HumanMessage(content=\"How many men survived?\")\n\n&gt;&gt;&gt; async for event in agent.astream_events(\n...     input={\n...         # After using checkpoint, you only need to add new messages here.\n...         \"messages\": [human_message],\n...         \"parent_id\": \"some-parent-id2\",\n...         \"date\": date.today(),  # noqa: DTZ011\n...     },\n...     version=\"v2\",\n...     # We configure the same thread_id to use checkpoints to retrieve the memory of the last run.\n...     config={\"configurable\": {\"thread_id\": \"some-thread-id\"}},\n... ):\n...     event_name: str = event[\"name\"]\n...     evt: str = event[\"event\"]\n...     if evt == \"on_chat_model_end\":\n...         print(event[\"data\"][\"output\"])\n...     elif event_name == \"tools\" and evt == \"on_chain_stream\":\n...         for lc_msg in event[\"data\"][\"chunk\"][\"messages\"]:\n...             print(lc_msg)\n...     else:\n...         # Other events can be handled here.\n...         pass\ncontent='\u8981\u786e\u5b9a\u6709\u591a\u5c11\u7537\u6027\u5e78\u5b58\uff0c\u6211\u4eec\u9700\u8981\u7b5b\u9009\u51fa\u6027\u522b\u4e3a \"male\" \u7684\u884c\uff0c\u5e76\u7edf\u8ba1\u8fd9\u4e9b\u884c\u4e2d `Survived` \u5217\u4e3a 1 \u7684\u6570\u91cf\u3002\u6211\u5c06\u6267\u884c\u76f8\u5e94\u7684\u4ee3\u7801\u6765\u5b8c\u6210\u8fd9\u4e2a\u4efb\u52a1\u3002\\n```python\\n# Filter the DataFrame to get only male passengers and count the survived ones\\nmale_survived_count = df[(df[\\'Sex\\'] == \\'male\\') &amp; (df[\\'Survived\\'] == 1)].shape[0]\\n\\nmale_survived_count\\n```' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-88035693-c17d-41ec-b8d1-b80373a5a5a1'\ncontent=[{'type': 'text', 'text': '```pycon\\n1\\n```'}] name='python' id='e05527a4-641c-4f26-a5c5-2e2960e23491' tool_call_id='02b9adc4-e2a7-49d1-9214-88d38104963d' artifact=[]\ncontent='\u5728\u63d0\u4f9b\u7684 Titanic \u6570\u636e\u96c6\u4e2d\uff0c\u6709 1 \u540d\u7537\u6027\u5e78\u5b58\u3002' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-49141ef6-818e-4837-9ddc-a77dff796e0e'\n</code></pre> Full code <pre><code>import asyncio\nfrom datetime import date\nfrom typing import TypedDict\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom pybox import LocalPyBoxManager\nfrom tablegpt.agent import create_tablegpt_graph\nfrom tablegpt.agent.file_reading import Stage\n\n\nclass Attachment(TypedDict):\n    \"\"\"Contains at least one dictionary with the key filename.\"\"\"\n\n    filename: str\n    \"\"\"The dataset uploaded in this session can be a filename, file path, or object storage address.\"\"\"\n\n\n# tablegpt-agent fully supports async invocation\nasync def main() -&gt; None:\n    llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\n\n    # Use local pybox manager for development and testing\n    pybox_manager = LocalPyBoxManager()\n\n    agent = create_tablegpt_graph(\n        llm=llm,\n        pybox_manager=pybox_manager,\n        # We use MemorySaver as a checkpointer to record memory automatically.\n        # See &lt;https://langchain-ai.github.io/langgraph/concepts/persistence&gt;\n        checkpointer=MemorySaver(),\n        # All code generated in this run will be executed in the kernel with kernel_id 'some-session-id'.\n        session_id=\"some-session-id\",\n    )\n\n    attachment_msg = HumanMessage(\n        content=\"\",\n        # The dataset can be viewed in examples/datasets/titanic.csv.\n        additional_kwargs={\"attachments\": [Attachment(filename=\"titanic.csv\")]},\n    )\n    await agent.ainvoke(\n        input={\n            \"entry_message\": attachment_msg,\n            \"processing_stage\": Stage.UPLOADED,\n            \"messages\": [attachment_msg],\n            \"parent_id\": \"some-parent-id1\",\n            \"date\": date.today(),  # noqa: DTZ011\n        },\n        config={\n            \"configurable\": {\"thread_id\": \"some-thread-id\"},\n        },\n    )\n\n    human_message = HumanMessage(content=\"How many men survived?\")\n\n    async for event in agent.astream_events(\n        input={\n            # After using checkpoint, you only need to add new messages here.\n            \"messages\": [human_message],\n            \"parent_id\": \"some-parent-id2\",\n            \"date\": date.today(),  # noqa: DTZ011\n        },\n        version=\"v2\",\n        # We configure the same thread_id to use checkpoints to retrieve the memory of the last run.\n        config={\"configurable\": {\"thread_id\": \"some-thread-id\"}},\n    ):\n        print(event)  # noqa: T201\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorials/continue-analysis-on-generated-charts/","title":"Continue Analysis on Generated Charts","text":"<p>While TableGPT2 excels in data analysis tasks, it currently lacks built-in support for visual modalities. Many data analysis tasks involve visualization, so to address this limitation, we provide an interface for integrating your own Visual Language Model (VLM) plugin.</p> <p>When the agent performs a visualization task\u2014typically using <code>matplotlib.pyplot.show</code>\u2014the VLM will take over from the LLM, offering a more nuanced summarization of the visualization. This approach avoids the common pitfalls of LLMs in visualization tasks, which often either state, \"I have plotted the data,\" or hallucinating the content of the plot.</p>"},{"location":"tutorials/quickstart/","title":"Quickstart","text":""},{"location":"tutorials/quickstart/#installation","title":"Installation","text":"<p>To install <code>tablegpt-agent</code>, use the following command:</p> <pre><code>pip install tablegpt-agent\n</code></pre> <p>This package depends on pybox, a Python code sandbox delegator. By default, <code>pybox</code> operates in an in-cluster mode. If you wish to run <code>tablegpt-agent</code> in a local environment, you need to install an optional dependency:</p> <pre><code>pip install tablegpt-agent[local]\n</code></pre>"},{"location":"tutorials/quickstart/#setup-llm-service","title":"Setup LLM Service","text":"<p>Before using <code>tablegpt-agent</code>, ensure that you have an OpenAI-compatible server set up to host TableGPT2. We recommend using vllm for this:</p> <p>Note:  If you need to use <code>tablegpt-agent</code> to analyze tabular data, please ensure your <code>vllm&gt;=0.5.5</code> <pre><code>pip install 'vllm&gt;=0.5.5'\n</code></pre></p> <pre><code>python -m vllm.entrypoints.openai.api_server --served-model-name TableGPT2-7B --model path/to/weights\n</code></pre> <p>Note: For production environments, it's important to optimize the vllm server configuration. For details, refer to the vllm documentation on server configuration.</p>"},{"location":"tutorials/quickstart/#chat-with-tablegpt-agent","title":"Chat with TableGPT Agent","text":"<p>To create a <code>TablegptAgent</code>, you'll need both an <code>LLM</code> and a <code>PyBoxManager</code> instance:</p> <p>NOTE The <code>llm</code> is created using <code>langchain-openai</code>, please install it first. <pre><code>pip install langchain-openai\n</code></pre></p> <pre><code>&gt;&gt;&gt; from langchain_openai import ChatOpenAI\n&gt;&gt;&gt; from pybox import LocalPyBoxManager\n&gt;&gt;&gt; from tablegpt.agent import create_tablegpt_graph\n&gt;&gt;&gt; from tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\n\n&gt;&gt;&gt; llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\n&gt;&gt;&gt; pybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\n\n&gt;&gt;&gt; agent = create_tablegpt_graph(\n...     llm=llm,\n...     pybox_manager=pybox_manager,\n... )\n</code></pre> <p>To interact with the agent:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from datetime import date\n&gt;&gt;&gt; from langchain_core.messages import HumanMessage\n\n&gt;&gt;&gt; message = HumanMessage(content=\"Hi\")\n\n&gt;&gt;&gt; _input = {\n...     \"messages\": [message],\n...     \"parent_id\": \"some-parent-id\",\n...     \"date\": date.today(),\n... }\n\n&gt;&gt;&gt; response = asyncio.run(agent.ainvoke(_input))\n&gt;&gt;&gt; print(response[\"messages\"])\n[HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}, id='7da7e51c-0ad0-4481-aa22-54acce6a82d7'), AIMessage(content=\"Hello! How can I assist you with your data analysis today? Do you have a dataset you'd like to work with?\", additional_kwargs={'parent_id': 'some-parent-id'}, response_metadata={}, id='43df249b-9c61-44bc-a535-eb33f9efaa9e')]\n</code></pre> <p>You can get more detailed outputs with the <code>astream_events</code> method:</p> <pre><code>&gt;&gt;&gt; async for event in agent.astream_events(\n...     input=_input,\n...     version=\"v2\",\n... ):\n...     # We ignore irrelevant events here.\n...     if event[\"event\"] == \"on_chat_model_end\":\n...         print(event[\"data\"][\"output\"])\ncontent=\"Hello! How can I assist you with your data analysis today? Do you have a specific dataset or problem in mind that you'd like to work on?\" additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'TableGPT2-7B'} id='run-677d42b7-ae79-4695-8e54-1b02fab07427'\n</code></pre> Full code <pre><code>import asyncio\nfrom datetime import date\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom pybox import LocalPyBoxManager\nfrom tablegpt.agent import create_tablegpt_graph\nfrom tablegpt import DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR\n\n\n# tablegpt-agent fully supports async invocation\nasync def main() -&gt; None:\n    llm = ChatOpenAI(openai_api_base=\"YOUR_VLLM_URL\", openai_api_key=\"whatever\", model_name=\"TableGPT2-7B\")\n\n    # Use local pybox manager for development and testing\n    pybox_manager = LocalPyBoxManager(profile_dir=DEFAULT_TABLEGPT_IPYKERNEL_PROFILE_DIR)\n\n    agent = create_tablegpt_graph(\n        llm=llm,\n        pybox_manager=pybox_manager,\n    )\n\n    message = HumanMessage(content=\"Hi\")\n    _input = {\n        \"messages\": [message],\n        \"parent_id\": \"some-parent-id\",\n        \"date\": date.today(),  # noqa: DTZ011\n    }\n\n    # response = await agent.ainvoke(_input)\n    # print(response[\"messages\"])\n\n    # More details can be obtained through the astream_events method\n    async for event in agent.astream_events(\n        input=_input,\n        version=\"v2\",\n    ):\n        print(event)  # noqa: T201\n\n\nasyncio.run(main())\n</code></pre>"}]}